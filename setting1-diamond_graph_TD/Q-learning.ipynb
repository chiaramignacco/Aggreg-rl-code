{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fee8bba-049f-4281-b450-f7620e30f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random \n",
    "import pickle\n",
    "from matplotlib.pyplot import figure\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stoc_match import StochasticMatching, Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5730098e-8676-4029-90bc-3bd7e16468fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# System and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e777f258-ba8c-4ede-a110-4c62b4a8736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "verteces = ['A', 'B', 'C', 'D']\n",
    "\n",
    "\n",
    "graph = {\n",
    "    'A': [('B', 200), ('C', 10), ('D', 50)],\n",
    "    'B': [('A', 200), ('D', 20)],\n",
    "    'C': [('A', 10), ('D', 1)],\n",
    "    'D': [('A', 50), ('B', 20), ('C', 1)]\n",
    "}\n",
    "\n",
    "arrival_rates = {\n",
    "    'A': 0.9,\n",
    "    'B': 0.2,\n",
    "    'C': 0.5,\n",
    "    'D': 0.6\n",
    "}\n",
    "\n",
    "arrival_rates_ind = {}\n",
    "for ind, vertex in zip(list(range(len(arrival_rates))), arrival_rates.keys()):\n",
    "    arrival_rates_ind[ind] = arrival_rates[vertex]\n",
    "\n",
    "    \n",
    "verteces_ind = {'A': 0,\n",
    "               'B': 1,\n",
    "               'C': 2,\n",
    "               'D': 3\n",
    "               }\n",
    "\n",
    "\n",
    "eta = 0.5\n",
    "discount = 0.8\n",
    "H = 50\n",
    "k = 10\n",
    "\n",
    "\n",
    "experts = ['match_the_longest', 'edge_priority_match_reward', 'random_match']\n",
    "queue_max = 5\n",
    "\n",
    "queues ={v: [] for v in verteces}\n",
    "\n",
    "\n",
    "state_space = list(itertools.product(np.array(range(queue_max + 1)), repeat=len(graph.keys())))\n",
    "\n",
    "n = 0\n",
    "state_space_ind = {}\n",
    "for s in state_space:\n",
    "    state_space_ind[str(n)] = s\n",
    "    n += 1 \n",
    "    \n",
    "exp_ind = {}\n",
    "n = 0\n",
    "for exp in experts:\n",
    "    exp_ind[str(n)] = exp\n",
    "    n += 1 \n",
    "    \n",
    "prob_arrival = []\n",
    "N =np.sum(np.array(list(arrival_rates.values())))\n",
    "\n",
    "for rate in arrival_rates.values():\n",
    "    prob_arrival.append(rate/N) \n",
    "    \n",
    "num_states = len(state_space)\n",
    "num_exp = len(experts)\n",
    "\n",
    "edges = [(u, v, w) for u in graph for v, w in graph[u]]\n",
    "edges_list = []\n",
    "\n",
    "    \n",
    "rewards_ind = {}\n",
    "\n",
    "for edge in edges:\n",
    "    e=[]\n",
    "    e.append(verteces_ind[np.array(edge)[0]])\n",
    "    e.append(verteces_ind[np.array(edge)[1]])\n",
    "    if [verteces_ind[np.array(edge)[1]], verteces_ind[np.array(edge)[0]]] not in edges_list:\n",
    "        edges_list.append(e)\n",
    "        rewards_ind[str(e)] = edge[2]\n",
    "    \n",
    "num_actions = len(edges_list) + 1\n",
    "\n",
    "state_action_space_ind = {}\n",
    "n = 0\n",
    "for s in state_space:\n",
    "    for a in edges_list:\n",
    "        state_action_space_ind[str(n)] = [s, a]\n",
    "        n += 1 \n",
    "\n",
    "key_list = list(state_space_ind.keys())\n",
    "val_list = list(state_space_ind.values())\n",
    "\n",
    "K = len(experts)    \n",
    "\n",
    "edges_ind = {}\n",
    "\n",
    "for edge, i in zip(edges_list, list(range(len(edges_list)))):\n",
    "    edges_ind[i] = edge\n",
    "edges_ind[len(edges_list)] = 'no_move'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e05c4f-f85f-44a2-841b-4ca3fee6c5fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06b6ac2-7777-4aef-84dc-bcd782f1a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_the_longest(edges_list, state):\n",
    "    selected_edges = []\n",
    "    edge_max = - np.inf\n",
    "    for edge in edges_list: \n",
    "        if (np.array(state)[edge[0]] > 0)  and (np.array(state)[edge[1]] > 0):\n",
    "            if (np.array(state)[edge[0]] + np.array(state)[edge[1]]) > edge_max: \n",
    "                edge_max = np.array(state)[edge[0]] + np.array(state)[edge[1]]\n",
    "                selected_edges = []\n",
    "                selected_edges.append(edge)\n",
    "            #elif np.array(state)[edge[0]] + np.array(state)[edge[1]] == edge_max:\n",
    "            #    selected_edges.append(edge)\n",
    "    return selected_edges\n",
    "\n",
    "def edge_priority_match_reward(edges_list, state):\n",
    "    selected_edges = []\n",
    "    edge_max = - np.inf\n",
    "    for edge in edges_list: \n",
    "        if (np.array(state)[edge[0]] > 0)  and (np.array(state)[edge[1]] > 0):\n",
    "            if rewards_ind[str(edge)] > edge_max: \n",
    "                edge_max = rewards_ind[str(edge)]\n",
    "                selected_edges = []\n",
    "                selected_edges.append(edge)\n",
    "            #elif rewards_ind[str(edge)] == edge_max:\n",
    "            #    selected_edges.append(edge)\n",
    "    return selected_edges\n",
    "\n",
    "def random_match(edges_list, state):\n",
    "    selected_edges = []\n",
    "    #edges_list_random = edges_list.copy()\n",
    "    #random.shuffle(edges_list_random)\n",
    "    for edge in edges_list: \n",
    "        if (np.array(state)[edge[0]] > 0)  and (np.array(state)[edge[1]] > 0):\n",
    "            #selected_edges = []\n",
    "            selected_edges.append(edge)\n",
    "            #break\n",
    "    return selected_edges\n",
    "\n",
    "def edge_priority_match_arrival_rate_low(edges_list, state):\n",
    "    selected_edges = []\n",
    "    edge_min = np.inf\n",
    "    for edge in edges_list: \n",
    "        if (np.array(state)[edge[0]] > 0)  and (np.array(state)[edge[1]] > 0):\n",
    "            if (arrival_rates_ind[edge[0]] + arrival_rates_ind[edge[1]]) < edge_min: \n",
    "                edge_min = arrival_rates_ind[edge[0]] + arrival_rates_ind[edge[1]] \n",
    "                selected_edges = []\n",
    "                selected_edges.append(edge)\n",
    "            #elif (arrival_rates_ind[edge[0]] + arrival_rates_ind[edge[1]]) == edge_min:\n",
    "            #    selected_edges.append(edge)\n",
    "    return selected_edges\n",
    "\n",
    "def edge_priority_match_arrival_rate_high(edges_list, state):\n",
    "    selected_edges = []\n",
    "    edge_max = - np.inf\n",
    "    for edge in edges_list: \n",
    "        if (np.array(state)[edge[0]] > 0)  and (np.array(state)[edge[1]] > 0):\n",
    "            if (arrival_rates_ind[edge[0]] + arrival_rates_ind[edge[1]]) > edge_max: \n",
    "                edge_max = arrival_rates_ind[edge[0]] + arrival_rates_ind[edge[1]] \n",
    "                selected_edges = []\n",
    "                selected_edges.append(edge)\n",
    "            #elif (arrival_rates_ind[edge[0]] + arrival_rates_ind[edge[1]]) == edge_max:\n",
    "            #    selected_edges.append(edge)\n",
    "    return selected_edges\n",
    "\n",
    "def edge_priority_match_random(edges_list, state):\n",
    "    selected_edges = []\n",
    "    edge_max = - np.inf\n",
    "    edges_list_ordered = edges_list.copy()\n",
    "    random.seed(11)\n",
    "    random.shuffle(edges_list_ordered)\n",
    "    for edge in edges_list_ordered: \n",
    "        if (np.array(state)[edge[0]] > 0)  and (np.array(state)[edge[1]] > 0):\n",
    "            arrival_rates_ind[edge[0]] + arrival_rates_ind[edge[1]] \n",
    "            selected_edges.append(edge)\n",
    "            break\n",
    "    return selected_edges\n",
    "\n",
    "experts_dict = {'match_the_longest': match_the_longest,\n",
    "                'edge_priority_match_reward' : edge_priority_match_reward,\n",
    "                'random_match': random_match,\n",
    "                \n",
    "                   } \n",
    "exp_ind = {'match_the_longest': 0,\n",
    "         'edge_priority_match_reward' : 1,\n",
    "         'random_match': 2,\n",
    "         } \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afbeb83-b6b7-4d0c-b35d-65ee2db68f55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Transition matrix and rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f998893-2bd3-4db6-8281-0f21b714ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cLI = (eta * queue_max * len(verteces))\n",
    "\n",
    "\n",
    "def compute_transitions_and_rewards_avg(weights):\n",
    "    P = np.zeros((num_states, num_states))\n",
    "    r = np.zeros(num_states)\n",
    "                       \n",
    "    for index, state in state_space_ind.items():\n",
    "        selected_edges_experts = {exp: [] for exp in experts} \n",
    "        for exp in experts: \n",
    "            selected_edges_experts[exp] = experts_dict[exp](edges_list, state)\n",
    "            if selected_edges_experts[exp]: \n",
    "                num_edges = len(selected_edges_experts[exp])\n",
    "                for edge in selected_edges_experts[exp]:\n",
    "                    q1 = edge[0]\n",
    "                    q2 = edge[1]\n",
    "                    new_state = np.array(state).copy()\n",
    "                    new_state[q1] -= 1\n",
    "                    new_state[q2] -= 1\n",
    "                    sum_queues = np.sum(new_state)\n",
    "                    for q in range(len(new_state)):\n",
    "                        if np.array(new_state)[q] < queue_max:\n",
    "                            new_state_after_arr = new_state.copy()\n",
    "                            new_state_after_arr[q] += 1\n",
    "                            new_state_after_arr = tuple(new_state_after_arr)\n",
    "                            index_new = int(val_list.index(new_state_after_arr))\n",
    "                            P[int(index), index_new] += weights[index][exp_ind[exp]] * (1 / num_edges) * prob_arrival[q]\n",
    "                            r[int(index)] += prob_arrival[q] * weights[index][exp_ind[exp]] * (1 / num_edges) * (rewards_ind[str(edge)] - eta * sum_queues)\n",
    "                        else:           \n",
    "                            new_state_after_arr = new_state.copy()\n",
    "                            index_new = int(val_list.index(tuple(new_state_after_arr)))\n",
    "                            P[int(index), index_new] += weights[index][exp_ind[exp]] * (1 / num_edges) * prob_arrival[q]\n",
    "                            r[int(index)] += weights[index][exp_ind[exp]] * (rewards_ind[str(edge)] - eta * sum_queues) * prob_arrival[q]* (1 / num_edges) \n",
    "            else:\n",
    "                for q in range(len(state)):\n",
    "                    if np.array(state)[q] < queue_max:\n",
    "                        new_state = np.array(state) \n",
    "                        new_state[q] += 1\n",
    "                        sum_queues = np.sum(state)\n",
    "                        new_state = tuple(new_state)\n",
    "                        index_new = int(val_list.index(new_state))\n",
    "                        P[int(index), index_new] += weights[index][exp_ind[exp]]  * prob_arrival[q]\n",
    "                        r[int(index)] += weights[index][exp_ind[exp]] * (- eta * sum_queues) * prob_arrival[q]\n",
    "                    else:           \n",
    "                        sum_queues = np.sum(state)\n",
    "                        P[int(index), int(index)] += weights[index][exp_ind[exp]] * prob_arrival[q]\n",
    "                        r[int(index)] += weights[index][exp_ind[exp]] * (- eta * sum_queues) * prob_arrival[q] \n",
    "    #normalise rewards\n",
    "    r_norm = r + cLI\n",
    "    # check\n",
    "    for s in range(len(state_space)):\n",
    "        if np.abs(np.sum(P[s,:]) - 1) > 1e-5:\n",
    "            print('Error: transition probabilities do not some to 1', np.sum(P[s,:]), s)\n",
    "        return P, r_norm\n",
    "    \n",
    "\n",
    "def compute_transitions_and_rewards_experts():\n",
    "    P = np.zeros((num_states, num_exp, num_states))\n",
    "    r = np.zeros((num_states, num_exp, num_states))\n",
    "    \n",
    "    for index, state in state_space_ind.items():\n",
    "        for exp in experts: \n",
    "            selected_edges_experts = experts_dict[exp](edges_list, state)\n",
    "            if selected_edges_experts: \n",
    "                num_edges = len(selected_edges_experts)\n",
    "                for edge in selected_edges_experts:\n",
    "                    q1 = edge[0]\n",
    "                    q2 = edge[1]\n",
    "                    new_state = np.array(state).copy()\n",
    "                    new_state[q1] -= 1\n",
    "                    new_state[q2] -= 1\n",
    "                    sum_queues = np.sum(new_state)\n",
    "                    for q in range(len(new_state)):\n",
    "                        if np.array(new_state)[q] < queue_max:\n",
    "                            new_state_after_arr = new_state.copy()\n",
    "                            new_state_after_arr[q] += 1\n",
    "                            new_state_after_arr = tuple(new_state_after_arr)\n",
    "                            index_new = int(val_list.index(new_state_after_arr))\n",
    "                            P[int(index), exp_ind[exp], index_new] += (1 / num_edges) * prob_arrival[q]\n",
    "                            r[int(index), exp_ind[exp], index_new] += (1 / num_edges) * (rewards_ind[str(edge)] - eta * sum_queues) \n",
    "                        else:            \n",
    "                            new_state_after_arr = new_state.copy()\n",
    "                            index_new = int(val_list.index(tuple(new_state_after_arr)))\n",
    "                            P[int(index), exp_ind[exp], index_new] += 1 / num_edges * prob_arrival[q]\n",
    "                            r[int(index), exp_ind[exp], index_new] += (1 / num_edges) *(rewards_ind[str(edge)] - eta * sum_queues)      \n",
    "            else:\n",
    "                for q in range(len(state)):\n",
    "                    if np.array(state)[q] < queue_max:\n",
    "                        new_state = np.array(state).copy()\n",
    "                        new_state[q] += 1\n",
    "                        sum_queues = np.sum(state)\n",
    "                        new_state = tuple(new_state)\n",
    "                        index_new = int(val_list.index(new_state))\n",
    "                        P[int(index),  exp_ind[exp], index_new] += prob_arrival[q]\n",
    "                        r[int(index),  exp_ind[exp], index_new] += - eta * sum_queues \n",
    "                    else:           \n",
    "                        sum_queues = np.sum(state)\n",
    "                        P[int(index), exp_ind[exp], int(index)] += prob_arrival[q]\n",
    "                        r[int(index), exp_ind[exp], int(index)] = (- eta * sum_queues)\n",
    "                        \n",
    "    #normalise rewards\n",
    "    r_norm = r + cLI\n",
    "    # check\n",
    "    for s in range(num_states):\n",
    "        for e in range(num_exp):\n",
    "            if np.abs(np.sum(P[s,e, :]) - 1) > 1e-5:\n",
    "                print('Error: transition probabilities do not some to 1', np.sum(P[s,:]), s)\n",
    "    return P, r_norm\n",
    "\n",
    "def compute_transitions_and_rewards_actions():\n",
    "    P = np.zeros((num_states, num_actions, num_states))\n",
    "    r = np.zeros((num_states, num_actions, num_states))\n",
    "    \n",
    "    for index, state in state_space_ind.items():\n",
    "        potential_matches = 0\n",
    "        for ind, edge in edges_ind.items():\n",
    "            if edge != 'no_move':\n",
    "                if (np.array(state)[edge[0]] > 0)  and (np.array(state)[edge[1]] > 0):\n",
    "                    potential_matches += 1\n",
    "                    q1 = edge[0]\n",
    "                    q2 = edge[1]\n",
    "                    new_state = np.array(state).copy()\n",
    "                    new_state[q1] -= 1\n",
    "                    new_state[q2] -= 1\n",
    "                    sum_queues = np.sum(new_state)\n",
    "                    for q in range(len(new_state)):\n",
    "                        if np.array(new_state)[q] < queue_max:\n",
    "                            new_state_after_arr = new_state.copy()\n",
    "                            new_state_after_arr[q] += 1\n",
    "                            new_state_after_arr = tuple(new_state_after_arr)\n",
    "                            index_new = int(val_list.index(new_state_after_arr))\n",
    "                            P[int(index),  ind, index_new] += prob_arrival[q]\n",
    "                            r[int(index), ind, index_new] +=  rewards_ind[str(edge)] - eta * sum_queues \n",
    "                        else:           \n",
    "                            index_new = int(val_list.index(tuple(new_state)))\n",
    "                            P[int(index), ind, index_new] += prob_arrival[q]\n",
    "                            r[int(index), ind, index_new] = rewards_ind[str(edge)] - eta * sum_queues\n",
    "    #check\n",
    "            elif edge == 'no_move':\n",
    "                if potential_matches==0:\n",
    "                    for q in range(len(state)):\n",
    "                        if np.array(state)[q] < queue_max:\n",
    "                            new_state = np.array(state) \n",
    "                            new_state[q] += 1\n",
    "                            sum_queues = np.sum(state)\n",
    "                            new_state = tuple(new_state)\n",
    "                            index_new = int(val_list.index(new_state))\n",
    "                            P[int(index),  ind, index_new] += prob_arrival[q]\n",
    "                            r[int(index),  ind, index_new] += - eta * sum_queues \n",
    "                        else:           \n",
    "                            sum_queues = np.sum(state)\n",
    "                            P[int(index), ind, int(index)] += prob_arrival[q]\n",
    "                            r[int(index), ind, int(index)] = - eta * sum_queues \n",
    "                        \n",
    "    #normalise rewards\n",
    "    r_norm = r + cLI\n",
    "    #check\n",
    "    for s in range(num_states):\n",
    "        for e in range(num_exp):\n",
    "            if (np.abs(np.sum(P[s,e, :]) - 1) > 1e-5) and (np.sum(P[s,e, :]) != 0):\n",
    "                print('Error: transition probabilities do not some to 1', np.sum(P[s,:]), s)\n",
    "    \n",
    "    return P, r_norm\n",
    "                        \n",
    "def compute_value_bellman(P, r):\n",
    "    I = np.eye(num_states)\n",
    "    inv = np.linalg.inv(I - discount*P)\n",
    "    V = np.matmul(inv, r)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564e5d9-c340-4722-9064-6d1a77532e4e",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad692b94-8d76-4092-8344-f8c76f56cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {state: np.array([1, 0, 0]) for state in state_space_ind.keys()}\n",
    "\n",
    "P_agg, r_agg = compute_transitions_and_rewards_avg(weights)\n",
    "V_agg = compute_value_bellman(P_agg, r_agg)\n",
    "\n",
    "Q_start = np.zeros((num_states, num_actions))\n",
    "for state in range(num_states):\n",
    "    Q_start[state, :] = V_agg[state] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc471959-a330-4f73-8b2c-88edd0382562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 50000000/50000000 [1:06:21<00:00, 12557.85it/s]\n",
      "100%|██████████████████████████| 50000000/50000000 [1:00:28<00:00, 13781.08it/s]\n",
      "100%|████████████████████████████| 50000000/50000000 [59:38<00:00, 13974.09it/s]\n",
      "100%|████████████████████████████| 50000000/50000000 [58:49<00:00, 14164.60it/s]\n",
      "100%|████████████████████████████| 50000000/50000000 [58:24<00:00, 14267.00it/s]\n",
      "100%|████████████████████████████| 50000000/50000000 [58:18<00:00, 14291.48it/s]\n",
      "100%|███████████████████████████| 50000000/50000000 [1:50:42<00:00, 7527.65it/s]\n",
      "100%|███████████████████████████| 50000000/50000000 [2:02:48<00:00, 6785.21it/s]\n",
      "100%|███████████████████████████| 50000000/50000000 [8:46:15<00:00, 1583.49it/s]\n",
      " 99%|███████████████████████████▌| 49286562/50000000 [58:10<00:49, 14287.92it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_episodes = 8000\n",
    "steps = 50000000\n",
    "num_actions = len(edges_list) + 1\n",
    "num_states = len(state_space)\n",
    "learning_rate = 1e-4\n",
    "start_learning_rate_decay = 1\n",
    "end_learning_rate_decay = steps \n",
    "learning_rate_decay_value = learning_rate / (end_learning_rate_decay - start_learning_rate_decay)\n",
    "\n",
    "repeat_ql = 10\n",
    "\n",
    "epsilon = 0.001\n",
    "start_epsilon_decay = 1\n",
    "end_epsilon_decay = steps # provare senza / 2\n",
    "epsilon_decay_value = epsilon / (end_epsilon_decay - start_epsilon_decay)\n",
    "\n",
    "model = StochasticMatching(graph, arrival_rates, queue_max)\n",
    "Q_evolution = []\n",
    "time_QL = []\n",
    "time_tot = 0\n",
    "res = 100000\n",
    "\n",
    "for n in range(repeat_ql):\n",
    "    Q = Q_start.copy()\n",
    "    Q_evolution = []\n",
    "    time_QL = []\n",
    "    time_tot = 0\n",
    "    epsilon = 0.1\n",
    "    learning_rate = 15e-5\n",
    "    model = StochasticMatching(graph, arrival_rates, queue_max)\n",
    "    state = int(model.val_list.index(state_space[0]))\n",
    "    for step in tqdm(range(steps)):\n",
    "        start_time = time.time()\n",
    "        new_Q, new_state = model.q_learning(Q, state, learning_rate, discount, epsilon, eta, steps)\n",
    "        end_time = time.time()\n",
    "        time_tot += end_time - start_time\n",
    "        Q = new_Q.copy()\n",
    "        state = new_state\n",
    "        if end_epsilon_decay >= step >= start_epsilon_decay:\n",
    "            epsilon -= epsilon_decay_value\n",
    "        if end_learning_rate_decay >= step >= start_learning_rate_decay:\n",
    "            learning_rate -= learning_rate_decay_value\n",
    "        if step%res == 0:\n",
    "            Q_max = 0\n",
    "            for i in range(len(verteces)):\n",
    "                s = np.zeros(len(verteces))\n",
    "                s[i] += 1\n",
    "                ind = str(val_list.index(tuple(s)))\n",
    "                Q_max += prob_arrival[i] * max(new_Q[int(ind)])\n",
    "            Q_evolution.append(Q_max)\n",
    "            time_QL.append(time_tot)\n",
    "            #if step % (res * 100) == 0:\n",
    "            #    print('step -> ',n, step)\n",
    "            \n",
    "    with open('weights/QL_' + str(n) + '.pkl', 'wb') as output:\n",
    "        pickle.dump(Q_evolution, output)\n",
    "    with open('weights/time_QL_' + str(n) + '.pkl', 'wb') as output:\n",
    "        pickle.dump(time_QL, output)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc173d-c88c-4b9e-b619-b2e4b2762fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_QL, Q_evolution, color='C5', linewidth=.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
